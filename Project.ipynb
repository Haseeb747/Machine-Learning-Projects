{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h5py) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
    "#have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review---\n",
      "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#A review from IMDB Dataset with its label\n",
    "print('---review---')\n",
    "print(train_data[6])\n",
    "print('---label---')\n",
    "print(train_labels[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:\n",
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len(max((train_data + test_data), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 70\n"
     ]
    }
   ],
   "source": [
    "print('Minimum review length: {}'.format(\n",
    "len(min((train_data + test_data), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to feed this data into our RNN, all input documents must have the same length.\n",
    "#  We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). \n",
    "# We can accomplish this using the pad_sequences() function in Keras. For now, set max_words to 500.\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "max_words = 500\n",
    "X_train = pad_sequences(train_data, maxlen=max_words)\n",
    "X_test = pad_sequences(test_data, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#To summarize, our model is a simple RNN model with 1 embedding, 1 LSTM and 1 dense layers. 213,301 parameters in total need to be trained.\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "390/390 [==============================] - 446s 1s/step - loss: 0.3027 - accuracy: 0.8771 - val_loss: 0.1579 - val_accuracy: 0.9375\n",
      "Epoch 2/3\n",
      "390/390 [==============================] - 388s 994ms/step - loss: 0.2105 - accuracy: 0.9216 - val_loss: 0.3388 - val_accuracy: 0.8594\n",
      "Epoch 3/3\n",
      "390/390 [==============================] - 443s 1s/step - loss: 0.1802 - accuracy: 0.9337 - val_loss: 0.2114 - val_accuracy: 0.9219\n"
     ]
    }
   ],
   "source": [
    "# X_valid & y_valid is the Testing data while X_train2 & y_train2 is the Training data\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:batch_size], train_labels[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], train_labels[batch_size:]\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.49900001287460327\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, train_labels, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   1,   19,   13,   35, 7110, 1187,  258,    9,   13,   52,  677,\n",
      "          2,  523,    8,    3, 1356, 5437])]\n",
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.79778624]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is not needed here, it is a practice and this section logic will be applied on user review data\n",
    "#print(predictions[0])\n",
    "#print(predictions[1])\n",
    "def encode(str):\n",
    "    import numpy as np\n",
    "    wordIndex = imdb.get_word_index()\n",
    "    list = []\n",
    "    str = str.split()\n",
    "    for words in str:\n",
    "        #print(words)\n",
    "        list.append(wordIndex[words])\n",
    "    #print(list)\n",
    "    list = np.array(list)\n",
    "    return list\n",
    "\n",
    "x = 'the film was so freaking awesome although it was very strange and directed in a noir mode'\n",
    "newList = encode(x)\n",
    "final_list = []\n",
    "final_list.append(newList)\n",
    "print(final_list)\n",
    "from keras.utils import pad_sequences\n",
    "max_words = 500\n",
    "final_list = pad_sequences(final_list, maxlen=max_words) \n",
    "#print(final_list)\n",
    "predictions = model.predict(final_list)\n",
    "predictions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
